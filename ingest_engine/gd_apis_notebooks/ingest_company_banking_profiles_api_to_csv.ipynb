{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the access to the GCP database, where we will store the listing data\n",
    "keyfile='gbq_json/project-pulldata-a0e9dfe25409.json'\n",
    "project_id = 'project-pulldata'\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    keyfile,\n",
    ")\n",
    "\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY LISTING --> API EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Company Listing\n",
    "api = 'GetCompanyListing'\n",
    "name = 'ns_media'\n",
    "token_id = 'hPP7@MubwL1C750pkWgUieNorBovxMsito/HZ9TilA4='\n",
    "\n",
    "page = 1\n",
    "url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}&PageNumber={page}&api_key={token_id}'\n",
    "response = requests.get(url).json()\n",
    "# assign the number of pages to a variable\n",
    "num_pages = response['NoOfPages']\n",
    "# instantiate an empty dataframe\n",
    "companies_df = pd.DataFrame()\n",
    "\n",
    "# in a loop over the 7 pages of the API results, fill the dataframe with the respective resutls\n",
    "for page in range(1, num_pages+1):\n",
    "    # apply the complete API call url\n",
    "    url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}&PageNumber={page}&api_key={token_id}'\n",
    "    # GET the response json\n",
    "    response = requests.get(url).json()\n",
    "    # parse the JSON to a pandas dataframe\n",
    "    curr_df = pd.DataFrame.from_dict(response['Records'])\n",
    "    # fill the dataframe above with each response\n",
    "    companies_df = pd.concat([companies_df, curr_df],sort=False)\n",
    "companies_df['PublishedDate'] = pd.to_datetime(companies_df['PublishedDate'])   \n",
    "companies_df.to_csv('BankingCompanyProfiles__CompanyListing_202008271110.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY DETAILS --> API EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Company Details\n",
    "\n",
    "api = 'GetCompanyDetails'\n",
    "name = 'ns_media'\n",
    "token_id = 'hPP7@MubwL1C750pkWgUieNorBovxMsito/HZ9TilA4'\n",
    "page = 1\n",
    "url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&PageNumber={page}&api_key={token_id}%3D'\n",
    "response = requests.get(url).json()\n",
    "# assign the number of pages to a variable\n",
    "num_pages = response['NoOfPages']\n",
    "# create an empty dataframe, which would contain all data for THIS city\n",
    "details_df = pd.DataFrame()\n",
    "\n",
    "# in a loop over the 3 pages of the API results, fill the dataframe with the respective resutls\n",
    "for page in range(1,num_pages+1):\n",
    "    # apply the complete API call url\n",
    "    url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&PageNumber={page}&api_key={token_id}%3D'\n",
    "    # GET the response json\n",
    "    response = requests.get(url).json()\n",
    "    # parse the JSON to a pandas dataframe\n",
    "    curr_df = pd.DataFrame.from_dict(response['Records'])\n",
    "    # fill the dataframe above with each response\n",
    "    details_df = pd.concat([details_df, curr_df],sort=False)\n",
    "details_df['PublishedDate'] = pd.to_datetime(details_df['PublishedDate'])\n",
    "original_table = details_df.iloc[:,:-10]\n",
    "original_table['SiteNames'] = details_df['SiteNames']\n",
    "original_table[['MajorProducts','MajorServices','MajorBrands']] = details_df[['MajorProducts','MajorServices','MajorBrands']]\n",
    "original_table.to_csv('BankingCompanyProfiles__CompanyDetails_202008271110.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect historical events\n",
    "total_history = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    history_table = json_normalize(individual['History'].tolist()[0])\n",
    "    history_table['CompanyId'] = individual.iloc[0,0]\n",
    "    total_history = pd.concat([total_history, history_table],sort=False)\n",
    "\n",
    "total_history.to_csv('BankingCompanyProfiles__HistoryDetails_202008251740.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect LocationSubsidiaries\n",
    "total_locationSubsidiaries = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    if(individual['LocationSubsidiaries'].tolist()[0] != None):\n",
    "        subsidiaries_table = json_normalize(individual['LocationSubsidiaries'].tolist()[0])\n",
    "        subsidiaries_table['SubsidiariesId'] = individual.iloc[0,0]\n",
    "        total_competitors = pd.concat([total_locationSubsidiaries, subsidiaries_table],sort=False)\n",
    "\n",
    "total_locationSubsidiaries.to_csv('BankingCompanyProfiles__LocationSubsidiariesDetails_202008251745.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect KeyTopcompetitors\n",
    "total_competitors = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    competitors_table = json_normalize(individual['KeyTopcompetitors'].tolist()[0])\n",
    "    competitors_table['CompetitorId'] = individual.iloc[0,0]\n",
    "    total_competitors = pd.concat([total_competitors, competitors_table],sort=False)\n",
    "\n",
    "total_competitors.to_csv('BankingCompanyProfiles__KeyTopCompetitorsDetails_202008251745.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect KeyEmployees\n",
    "total_employees = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    employee_table = json_normalize(individual['KeyEmployees'].tolist()[0])\n",
    "    employee_table['CompanyId'] = individual.iloc[0,0]\n",
    "    total_employees = pd.concat([total_employees, employee_table],sort=False)\n",
    "\n",
    "total_employees.to_csv('BankingCompanyProfiles__KeyEmployeesDetails_202008251750.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect KeyFacts\n",
    "total_facts = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    facts_table = json_normalize(individual['KeyFacts'].tolist()[0])\n",
    "    facts_table['CompanyId'] = individual.iloc[0,0]\n",
    "    total_facts = pd.concat([total_facts, facts_table],sort=False)\n",
    "\n",
    "total_facts.to_csv('BankingCompanyProfiles__KeyFactsDetails_202008251755.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing: SwotAnalysis(companyid and Overview) which needs to be split in: SwotAnalysis_Strengths,\n",
    "## SwotAnalysis_Weakness,SwotAnalysis_Opprotunities,SwotAnalysis_Threats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from details_df collect SwotAnalysis\n",
    "total_swot = pd.DataFrame()\n",
    "for company in details_df['CompanyId'].tolist():\n",
    "    individual = details_df[details_df['CompanyId'] == company]\n",
    "    swot_table = json_normalize(individual['SwotAnalysis'].tolist()[0])\n",
    "    swot_table['CompanyId'] = individual.iloc[0,0]\n",
    "    total_swot = pd.concat([total_swot, swot_table],sort=False)\n",
    "    total_swot1 = total_swot[['Overview','CompanyId']]\n",
    "total_swot1.to_csv('BankingCompanyProfiles__SwotAnalysisDetails_202008260900.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from SwotAnalysis collect Strengths\n",
    "total_strengths = pd.DataFrame()\n",
    "for company in total_swot['CompanyId'].tolist():\n",
    "    individual = total_swot[total_swot['CompanyId'] == company]\n",
    "    strengths = json_normalize(individual['Strengths'].tolist()[0])\n",
    "    strengths['CompanyId'] = company\n",
    "    total_strengths = pd.concat([total_strengths, strengths],sort=False)\n",
    "total_strengths.to_csv('BankingCompanyProfiles__StrengthsSwotAnalysisDetails_202008261615.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from SwotAnalysis collect Weakness\n",
    "\n",
    "total_weakness = pd.DataFrame()\n",
    "for company in total_swot['CompanyId'].tolist():\n",
    "    individual = total_swot[total_swot['CompanyId'] == company]\n",
    "    weakness = json_normalize(individual['Weakness'].tolist()[0])\n",
    "    weakness['CompanyId'] = company\n",
    "    total_weakness = pd.concat([total_weakness, weakness],sort=False)\n",
    "total_weakness.to_csv('BankingCompanyProfiles__WeaknessSwotAnalysisDetails_202008260.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from SwotAnalysis collect Opprotunities\n",
    "total_opprotunities = pd.DataFrame()\n",
    "for company in total_swot['CompanyId'].tolist():\n",
    "    individual = total_swot[total_swot['CompanyId'] == company]\n",
    "    opportunities = json_normalize(individual['Opprotunities'].tolist()[0])\n",
    "    opportunities['CompanyId'] = individual.iloc[0,0]\n",
    "    total_opprotunities = pd.concat([total_opprotunities, opportunities],sort=False)\n",
    "total_opprotunities.to_csv('BankingCompanyProfiles__OpportunitiesSwotAnalysisDetails_202008260915.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each company from SwotAnalysis collect Threats\n",
    "total_threats = pd.DataFrame()\n",
    "for company in total_swot['CompanyId'].tolist():\n",
    "    individual = total_swot[total_swot['CompanyId'] == company]\n",
    "    threats = json_normalize(individual['Threats'].tolist()[0])\n",
    "    threats['CompanyId'] = individual.iloc[0,0]\n",
    "    total_threats = pd.concat([total_threats, threats],sort=False)\n",
    "total_threats.to_csv('BankingCompanyProfiles__ThreatsSwotAnalysisDetails_202008260915.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAL DETAILS --> API EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Deal Details\n",
    "\n",
    "api = 'GetDealDetails'\n",
    "name = 'ns_media'\n",
    "token_id = 'hPP7@MubwL1C750pkWgUieNorBovxMsito/HZ9TilA4'\n",
    "\n",
    "deal_details = pd.DataFrame()\n",
    "#for each CompanyId\n",
    "for company in companies_df['CompanyId'].tolist():\n",
    "    page = 1\n",
    "    ## call the api and collect number of pages\n",
    "    url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&CompanyID={company}&api_key={token_id}%3D'\n",
    "    # GET the response json\n",
    "    response = requests.get(url).json()\n",
    "    num_pages = response['NoOfPages']\n",
    "    #print('Company: ',company,' pages: ' , num_pages, '\\n')\n",
    "    # in a loop over the num_pages pages of the API results, fill the dataframe with the respective resutls\n",
    "    for page in range(1,num_pages+1):\n",
    "        # apply the complete API call url\n",
    "        url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&CompanyID={company}&PageNumber={page}&api_key={token_id}%3D'\n",
    "        # GET the response json\n",
    "        response = requests.get(url).json()\n",
    "        # parse the JSON to a pandas dataframe\n",
    "        curr_df = pd.DataFrame.from_dict(response['Records'])\n",
    "        curr_df['CompanyId'] = company\n",
    "        # fill the dataframe above with each response\n",
    "        deal_details = pd.concat([deal_details, curr_df], sort=False)\n",
    "deal_details['DealID'] = deal_details['DealID'].astype('int32')\n",
    "## Fixing date 0001-01-01\n",
    "for date in deal_details['DealCompletedDate'].tolist():\n",
    "    if (date == '0001-01-01T00:00:00') :\n",
    "        (deal_details['DealCompletedDate'][deal_details['DealCompletedDate'] == date]) = None        \n",
    "for date in deal_details['AnnounceDate'].tolist():\n",
    "    if (date == '0001-01-01T00:00:00') :\n",
    "        (deal_details['AnnounceDate'][deal_details['AnnounceDate'] == date]) = None\n",
    "for date in deal_details['OptionalDateValue'].tolist():\n",
    "    if (date == '0001-01-01T00:00:00') :\n",
    "        (deal_details['OptionalDateValue'][deal_details['OptionalDateValue'] == date]) = None\n",
    "        \n",
    "deal_details['DealCompletedDate'] = pd.to_datetime(deal_details['DealCompletedDate'])\n",
    "deal_details['PublishedDate'] = pd.to_datetime(deal_details['PublishedDate'])\n",
    "deal_details['AnnounceDate'] = pd.to_datetime(deal_details['AnnounceDate'])\n",
    "deal_details['RumorDate'] = pd.to_datetime(deal_details['RumorDate'])\n",
    "deal_details['OptionalDateValue'] = pd.to_datetime(deal_details['OptionalDateValue'])\n",
    "\n",
    "\n",
    "original_deals = deal_details[['DealID','Title','Description','DealCompletedDate','DealCompletedYear','DealCompletedQuarter',\n",
    "                               'DealStatus','DealType','DealValue','DealRationale','RoundofFinancing','DealCountry',\n",
    "                               'UrlNode','PublishedDate','AnnounceDate','DealSubType','DealSubTypeL1','DealSubTypeL2',\n",
    "                               'DealSubTypeL3','AcquiredStake','LocalCurrency','AssetName','RumorDate','OptionalDateName',\n",
    "                               'OptionalDateValue','Sources','CompanyId']]\n",
    "original_deals.to_csv('BankingCompanyProfiles__DealDetails_202008271240.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect TargetAssets\n",
    "total_targetassets = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['TargetAssets'].tolist()[0] != None):\n",
    "        targetAssets = json_normalize(individual['TargetAssets'].tolist()[0])\n",
    "        targetAssets['DealID'] = individual.iloc[0,0]\n",
    "        total_targetassets = pd.concat([total_targetassets, targetAssets],sort=False)\n",
    "total_targetassets.to_csv('BankingCompanyProfiles__TargetAssetsDealDetails_202008261120.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-02953eb9cdbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdeal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindividual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdeal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# loop for each deal from deals collect Targets\n",
    "total_targets = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['Targets'].tolist()[0] != None):\n",
    "        targets = json_normalize(individual['Targets'].tolist()[0])\n",
    "        targets['DealID'] = individual.iloc[0,0]\n",
    "        total_targets = pd.concat([total_targets, targets],sort=False)\n",
    "total_targets['CompanyId'] = total_targets['CompanyId'].astype('int32')\n",
    "total_targets.to_csv('BankingCompanyProfiles__TargetsDealDetails_202008261120.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect Aquirers\n",
    "total_aquirers = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['Aquirers'].tolist()[0] != None):\n",
    "        aquirers = json_normalize(individual['Aquirers'].tolist()[0])\n",
    "        aquirers['DealID'] = individual.iloc[0,0]\n",
    "        total_aquirers = pd.concat([total_aquirers, aquirers], sort=False)\n",
    "total_aquirers['CompanyId'] = total_aquirers['CompanyId'].astype('int32')\n",
    "total_aquirers.to_csv('BankingCompanyProfiles__AquirersDealDetails_202008261125.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect Vendors\n",
    "total_vendors = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['Vendors'].tolist()[0] != None):\n",
    "        vendors = json_normalize(individual['Vendors'].tolist()[0])\n",
    "        vendors['DealID'] = individual.iloc[0,0]\n",
    "        total_vendors = pd.concat([total_vendors, vendors], sort=False)\n",
    "total_vendors['CompanyId'] = total_vendors['CompanyId'].astype('int32')\n",
    "total_vendors.to_csv('BankingCompanyProfiles__VendorsDealDetails_202008261125.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{None}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# MajorProducts,MajorServices,MajorBrands,LocationSubsidiaries\n",
    "columnlist1 = set(deal_details['Vendors'].tolist())\n",
    "print(columnlist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-44131fdf545f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdeal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindividual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeal_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdeal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DealID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# loop for each deal from deals collect Entities\n",
    "total_entities = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['Entities'].tolist()[0] != None):\n",
    "        entities = json_normalize(individual['Entities'].tolist()[0])\n",
    "        entities['DealID'] = individual.iloc[0,0]\n",
    "        total_entities = pd.concat([total_entities, entities], sort=False)\n",
    "total_entities.to_csv('BankingCompanyProfiles__EntitiesDealDetails_202008261125.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect FinacialAdvisors\n",
    "total_finacialAdvisors = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['FinacialAdvisors'].tolist()[0] != None):\n",
    "        FinacialAdvisors = json_normalize(individual['FinacialAdvisors'].tolist()[0])\n",
    "        FinacialAdvisors['DealID'] = individual.iloc[0,0]\n",
    "        total_finacialAdvisors = pd.concat([total_finacialAdvisors, FinacialAdvisors], sort=False)\n",
    "total_finacialAdvisors.to_csv('BankingCompanyProfiles__FinacialAdvisorsDealDetails_202008261130.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect LegalAdvisors\n",
    "total_legalAdvisors = pd.DataFrame()\n",
    "for deal in deal_details['DealID'].tolist():\n",
    "    individual = deal_details[deal_details['DealID'] == deal]\n",
    "    if(individual['LegalAdvisors'].tolist()[0] != None):\n",
    "        LegalAdvisors = json_normalize(individual['LegalAdvisors'].tolist()[0])\n",
    "        LegalAdvisors['DealID'] = individual.iloc[0,0]\n",
    "        total_legalAdvisors = pd.concat([total_legalAdvisors, LegalAdvisors], sort=False)\n",
    "total_legalAdvisors.to_csv('BankingCompanyProfiles__LegalAdvisorsDealDetails_202008261135.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS DETAILS --> API EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## News Details\n",
    "\n",
    "api = 'GetNewsDetails'\n",
    "name = 'ns_media'\n",
    "token_id = 'hPP7@MubwL1C750pkWgUieNorBovxMsito/HZ9TilA4'\n",
    "\n",
    "news_details = pd.DataFrame()\n",
    "#for each CompanyId\n",
    "for company in companies_df['CompanyId'].tolist():\n",
    "    page = 1\n",
    "    ## call the api and collect number of pages\n",
    "    url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&CompanyID={company}&api_key={token_id}%3D'\n",
    "    # GET the response json\n",
    "    response = requests.get(url).json()\n",
    "    num_pages = response['NoOfPages']\n",
    "    #print('Company: ',company,' pages: ' , num_pages, '\\n')\n",
    "    # in a loop over the num_pages pages of the API results, fill the dataframe with the respective resutls\n",
    "    for page in range(1,num_pages+1):\n",
    "        # apply the complete API call url\n",
    "        url = f'http://apidata.globaldata.com/CompanyIntegratedViewNSMG/api/Content/{api}?TokenID={token_id}%3D&CompanyID={company}&PageNumber={page}&api_key={token_id}%3D'     \n",
    "        # GET the response json\n",
    "        response = requests.get(url).json()\n",
    "        # parse the JSON to a pandas dataframe\n",
    "        curr_df = pd.DataFrame.from_dict(response['Records'])\n",
    "        curr_df['CompanyId'] = company\n",
    "        # fill the dataframe above with each response\n",
    "        news_details = pd.concat([news_details, curr_df], sort=False)\n",
    "news_details['PublishedDate'] = pd.to_datetime(news_details['PublishedDate'])\n",
    "original_news = news_details.drop(columns=['NewsArticleCompanies'])\n",
    "original_news.to_csv('BankingCompanyProfiles__NewsDetails_202008271210.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for each deal from deals collect NewsArticleCompanies\n",
    "total_NewsArticleCompanies = pd.DataFrame()\n",
    "for new in news_details['NewsArticleId'].tolist():\n",
    "    individual = news_details[news_details['NewsArticleId'] == new]\n",
    "    if(individual['NewsArticleCompanies'].tolist()[0] != None):\n",
    "        newsArticleCompanies = json_normalize(individual['NewsArticleCompanies'].tolist()[0])\n",
    "        newsArticleCompanies['NewsArticleId'] = individual.iloc[0,0]\n",
    "        total_NewsArticleCompanies = pd.concat([total_NewsArticleCompanies, newsArticleCompanies], sort=False)\n",
    "total_NewsArticleCompanies.to_csv('BankingCompanyProfiles__NewsArticleCompaniesDetails_202008261155.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
