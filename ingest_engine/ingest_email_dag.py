from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
import pandas as pd
import datetime
from datetime import date, timedelta
import os
from collections import defaultdict
from ingest_utils.constants import LOGS_FOLDER, API_TEST_LOG, CLEANUP_LOG, UPDATE_PROCESSING_RESULTS, \
    UPDATE_COVID19_PROCESSING_RESULTS, UPDATE_COMPANIES_PROCESSING_RESULTS, UPDATE_MNC_PROCESSING_RESULTS, \
    UPDATE_SURVEYS_PROCESSING_RESULTS, PROCESSING_RESULTS

from ingest_email.constants import EMAIL_LOG_FILE
from ingest_utils.logging import log_last_run_date


def create_email_content() -> str:
    """
    Read multiple logs generated by different dags and create the email content
    :return: actual email message
    """
    # read email log to find the last sent email
    df_time = pd.read_csv(f'{LOGS_FOLDER}{EMAIL_LOG_FILE}', header=None)
    df_time[0] = pd.to_datetime(df_time[0]).dt.date
    # get last_day_check date
    last_day_check = date.today() - timedelta(days=1)
    # list all log file in ingest logs folder
    logs_dir = os.listdir(LOGS_FOLDER)
    # keep those logs, which should be processed by the email dag
    email_file_list = [list_item for list_item in logs_dir if list_item.endswith('processing_results.csv')]
    email_file_list.extend([API_TEST_LOG, CLEANUP_LOG])
    # create new default dict to store all dataframes
    dict_results = defaultdict()
    # for each file
    for file in email_file_list:
        # read the file with | separator
        df = pd.read_csv(LOGS_FOLDER + file, header=None, sep='|')
        # for the log file from commercial updates:
        if UPDATE_PROCESSING_RESULTS in file:
            # count number of processed entities
            df['len'] = df[1].apply(lambda x: len(str(x).split(',')))
            # get the entity
            df['city_or_country'] = df[2].apply(lambda x: str(x).split('.')[0])
            # concatenate findings
            df['concat'] = df['city_or_country'] + ': ' + df['len'].astype('str')
            # remove unnecessary columns
            df = df.drop(columns=[1, 3, 'len', 'city_or_country'], axis=1)
        # rename all columns
        df.columns = ['Timestamp', 'Filename', 'Message']
        # convert Timestamp string to Timestamp object
        df['Timestamp'] = pd.to_datetime(df['Timestamp'], infer_datetime_format=True)
        # get date from Timestamp object
        df['Date'] = df['Timestamp'].dt.date
        # keep only valid entries
        df_new = df[df['Date'] > df_time.iloc[0, 0]]
        # add the results to a dictionary
        dict_results[file] = df_new
    # create response
    response = "<h4>Report on " + str(last_day_check) + ":</h4>"
    # add findings for API test
    df = dict_results[API_TEST_LOG]
    if df.shape[0] == 0:
        response = response + '<p>No API test was performed.</p>'
    else:
        response = response + '<p>' + df['Message'].values[0] + '</p>'
    # add findings for Cleanup API
    df = dict_results[CLEANUP_LOG]
    if df.shape[0] == 0:
        response = response + '<p>No cleanup was performed.</p>'
    else:
        response = response + '<p>' + df['Message'].values[0] + '</p>'
    # add findings for commercial updates
    df = dict_results[UPDATE_PROCESSING_RESULTS]
    if df.shape[0] == 0:
        response = response + '<p>No updates from Economics API.</p>'
    else:
        response = response + '<p>' + 'Economics API update:' + str(set(df['Message'])) + '</p>'
    # add findings for Covid19 data update
    df = dict_results[UPDATE_COVID19_PROCESSING_RESULTS]
    if df.shape[0] == 0:
        response = response + '<p>No updates from Covid19 data.</p>'
    else:
        response = response + '<p>' + str(
            len(set(df['Filename']))) + ' Covid19 table(s) were updated successfully.' + '</p>'
    # add findings for Companies API updates
    df_raw = dict_results[UPDATE_COMPANIES_PROCESSING_RESULTS]
    if df_raw.shape[0] == 0:
        response = response + '<p>No updates from Companies API.</p>'
    else:
        # get number of successfully updated tables
        df_succ = df_raw[df_raw['Message'].str.endswith('successfully.')]
        succ = len(set(df_succ['Filename']))
        # get number of skipped tables
        df_fail = df_raw[~df_raw['Message'].str.endswith('successfully.')]
        fail = len(set(df_fail['Filename']))
        response = response + '<p>' + str(succ) + ' Companies table(s) were updated successfully. ' + str(
            fail) + ' Companies table(s) were not updated.</p>'
    # add findings for MNC API updates
    df_raw = dict_results[UPDATE_MNC_PROCESSING_RESULTS]
    if df_raw.shape[0] == 0:
        response = response + '<p>No updates from MNC Segments and Subsidiaries API.</p>'
    else:
        # get number of successfully updated tables
        df_succ = df_raw[df_raw['Message'].str.endswith('successfully.')]
        succ = len(set(df_succ['Filename']))
        # get number of skipped tables
        df_fail = df_raw[~df_raw['Message'].str.endswith('successfully.')]
        fail = len(set(df_fail['Filename']))
        response = response + '<p>' + str(succ) + ' MNC table(s) were updated successfully. ' + str(
            fail) + ' MNC table(s) were not updated.</p>'
    # add findings for Alchemer API updates
    df_raw = dict_results[UPDATE_SURVEYS_PROCESSING_RESULTS]
    if df_raw.shape[0] == 0:
        response = response + '<p>No updates from Alchemer API.</p>'
    else:
        # get number of successfully updated tables
        df_succ = df_raw[df_raw['Message'].str.endswith('successfully.')]
        succ = len(set(df_succ['Filename']))
        # get number of skipped tables
        df_fail = df_raw[~df_raw['Message'].str.endswith('successfully.')]
        fail = len(set(df_fail['Filename']))
        response = response + '<p>' + str(succ) + \
                   ' Alchemer table(s) were updated successfully. ' \
                   + str(fail) + ' Alchemer table(s) were not updated.</p>'
    # add findings for Ingest
    df = dict_results[PROCESSING_RESULTS]
    if df.shape[0] == 0:
        response = response + '<p>No new file was detected after check on date: ' + str(df_time.iloc[0, 0]) + '</p>'
    else:
        response = response + '<p>The files were recorded from date ' \
                   + str(df_time.iloc[0, 0] + timedelta(days=1)) + \
                   " to " + str(last_day_check) + ":</p>" + df.to_html() + '</p>'

    return response


default_args = {
    'owner': 'email_processing_logs',
    'start_date': datetime.datetime(2020, 7, 3, 5, 1, 00),
    'concurrency': 1,
    'retries': 0
}

dag = DAG('email_processing_logs', description='Email processed logs',
          schedule_interval='0 5 * * *',
          default_args=default_args, catchup=False)

create_email_content = PythonOperator(task_id='create_email_content', python_callable=create_email_content, dag=dag)

email = EmailOperator(
    task_id='send_email',
    to=['petyo.karatov@ns-mediagroup.com', 'veselin.valchev@ns-mediagroup.com', 'nina.nikolaeva@ns-mediagroup.com',
        'yavor.vrachev@ns-mediagroup.com', 'Dzhumle.Mehmedova@ns-mediagroup.com'],
    subject='Dag Performed Operations Log',
    html_content="""<h3>Dag Performed Operations Log:</h3><div> {{ti.xcom_pull(task_ids='create_email_content')}} </div>""",
    dag=dag
)

record_email_log = PythonOperator(task_id='log_last_run_date',
                                  python_callable=log_last_run_date,
                                  op_kwargs={"log_file": f'{LOGS_FOLDER}{EMAIL_LOG_FILE}'},
                                  dag=dag)

create_email_content >> email >> record_email_log
